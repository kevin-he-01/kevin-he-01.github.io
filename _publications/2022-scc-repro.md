---
title: "Critique of: “A Parallel Framework for Constraint-Based Bayesian Network Learning via Markov Blanket Discovery” by SCC Team from UC San Diego"
authors: 'Arunav Gupta, John Ge, John Li, Zihao Kong, <u>Kaiwen He</u>, Matthew Mikhailov, Bryan Chin, Xiaochen Li, Max Apodaca, Paul Rodriguez, Mahidar Tatineni, Mary Thomas, and Santosh Bhatt.'
collection: publications
category: manuscripts
permalink: /publication/2022-scc-repro
excerpt: "Bayesian networks (BNs) have become popular in recent years to describe natural phenomena in situations where causal linkages are important to understand. In order to get around the inherent non-tractability of learning BNs, Srivastava et al. propose a markov blanket discovery-based approach to learning in their paper titled “A Parallel Framework for Constraint-based Bayesian Network Learning via Markov Blanket Discovery.” We are able to reproduce both the strong and weak scaling experiments from the paper up to 128 cores, and verify communication cost scaling for all three algorithms in the paper. We also introduce methodological improvements to weak scaling that show the paper's findings are unique to the methodology and not the datasets used. Slight variations in performance were observed due to differences in datasets, core count, and job scheduling."
date: 2022-10-31
venue: 'IEEE TPDS'
paperurl: 'https://ieeexplore.ieee.org/iel7/71/10123122/09933728.pdf'
---

Bayesian networks (BNs) have become popular in recent years to describe natural phenomena in situations where causal linkages are important to understand. In order to get around the inherent non-tractability of learning BNs, Srivastava et al. propose a markov blanket discovery-based approach to learning in their paper titled “A Parallel Framework for Constraint-based Bayesian Network Learning via Markov Blanket Discovery.” We are able to reproduce both the strong and weak scaling experiments from the paper up to 128 cores, and verify communication cost scaling for all three algorithms in the paper. We also introduce methodological improvements to weak scaling that show the paper's findings are unique to the methodology and not the datasets used. Slight variations in performance were observed due to differences in datasets, core count, and job scheduling.
